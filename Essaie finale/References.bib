
@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
            
              One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts
              et al.
              describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
            
            
              Science
              , this issue
              10.1126/science.aac4716
            
          , 
            A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
          , 
            
              INTRODUCTION
              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
            
            
              RATIONALE
              There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
            
            
              RESULTS
              
                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and
                P
                values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (
                M
                r
                = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (
                M
                r
                = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (
                P
                {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
              
            
            
              CONCLUSION
              
                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original
                P
                value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.
              
              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.
              
                
                  Original study effect size versus replication effect size (correlation coefficients).
                  Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
                
                
              
            
          , 
            Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	language = {en},
	number = {6251},
	urldate = {2022-04-22},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pages = {aac4716},
	file = {Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:C\:\\Users\\jorda\\Zotero\\storage\\WFSWP5R7\\Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf},
}

@article{london_against_2020,
	title = {Against pandemic research exceptionalism},
	volume = {368},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.abc1731},
	doi = {10.1126/science.abc1731},
	abstract = {Crises are no excuse for lowering scientific standards
          , 
            
              The global outbreak of coronavirus disease 2019 (COVID-19) has seen a deluge of clinical studies, with hundreds registered on
              clinicaltrials.gov
              . But a palpable sense of urgency and a lingering concern that “in critical situations, large randomized controlled trials are not always feasible or ethical” (
              1
              ) perpetuate the perception that, when it comes to the rigors of science, crisis situations demand exceptions to high standards for quality. Early phase studies have been launched before completion of investigations that would normally be required to warrant further development of the intervention (
              2
              ), and treatment trials have used research strategies that are easy to implement but unlikely to yield unbiased effect estimates. Numerous trials investigating similar hypotheses risk duplication of effort, and droves of research papers have been rushed to preprint servers, essentially outsourcing peer review to practicing physicians and journalists. Although crises present major logistical and practical challenges, the moral mission of research remains the same: to reduce uncertainty and enable caregivers, health systems, and policy-makers to better address individual and public health. Rather than generating permission to carry out low-quality investigations, the urgency and scarcity of pandemics heighten the responsibility of key actors in the research enterprise to coordinate their activities to uphold the standards necessary to advance this mission.},
	language = {en},
	number = {6490},
	urldate = {2022-04-22},
	journal = {Science},
	author = {London, Alex John and Kimmelman, Jonathan},
	month = may,
	year = {2020},
	pages = {476--477},
	file = {Texte intégral:C\:\\Users\\jorda\\Zotero\\storage\\WDHW93ZV\\London et Kimmelman - 2020 - Against pandemic research exceptionalism.pdf:application/pdf},
}

@article{mills_archiving_2015,
	title = {Archiving {Primary} {Data}: {Solutions} for {Long}-{Term} {Studies}},
	volume = {30},
	issn = {01695347},
	shorttitle = {Archiving {Primary} {Data}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169534715001858},
	doi = {10.1016/j.tree.2015.07.006},
	language = {en},
	number = {10},
	urldate = {2022-04-22},
	journal = {Trends in Ecology \& Evolution},
	author = {Mills, James A. and Teplitsky, Céline and Arroyo, Beatriz and Charmantier, Anne and Becker, Peter. H. and Birkhead, Tim R. and Bize, Pierre and Blumstein, Daniel T. and Bonenfant, Christophe and Boutin, Stan and Bushuev, Andrey and Cam, Emmanuelle and Cockburn, Andrew and Côté, Steeve D. and Coulson, John C. and Daunt, Francis and Dingemanse, Niels J. and Doligez, Blandine and Drummond, Hugh and Espie, Richard H.M. and Festa-Bianchet, Marco and Frentiu, Francesca and Fitzpatrick, John W. and Furness, Robert W. and Garant, Dany and Gauthier, Gilles and Grant, Peter R. and Griesser, Michael and Gustafsson, Lars and Hansson, Bengt and Harris, Michael P. and Jiguet, Frédéric and Kjellander, Petter and Korpimäki, Erkki and Krebs, Charles J. and Lens, Luc and Linnell, John D.C. and Low, Matthew and McAdam, Andrew and Margalida, Antoni and Merilä, Juha and Møller, Anders P. and Nakagawa, Shinichi and Nilsson, Jan-Åke and Nisbet, Ian C.T. and van Noordwijk, Arie J. and Oro, Daniel and Pärt, Tomas and Pelletier, Fanie and Potti, Jaime and Pujol, Benoit and Réale, Denis and Rockwell, Robert F. and Ropert-Coudert, Yan and Roulin, Alexandre and Sedinger, James S. and Swenson, Jon E. and Thébaud, Christophe and Visser, Marcel E. and Wanless, Sarah and Westneat, David F. and Wilson, Alastair J. and Zedrosser, Andreas},
	month = oct,
	year = {2015},
	pages = {581--589},
	file = {Mills et al. - 2015 - Archiving Primary Data Solutions for Long-Term St.pdf:C\:\\Users\\jorda\\Zotero\\storage\\P2NNXMP5\\Mills et al. - 2015 - Archiving Primary Data Solutions for Long-Term St.pdf:application/pdf},
}

@article{milcu_genotypic_2018,
	title = {Genotypic variability enhances the reproducibility of an ecological study},
	volume = {2},
	issn = {2397-334X},
	url = {http://www.nature.com/articles/s41559-017-0434-x},
	doi = {10.1038/s41559-017-0434-x},
	language = {en},
	number = {2},
	urldate = {2022-04-22},
	journal = {Nature Ecology \& Evolution},
	author = {Milcu, Alexandru and Puga-Freitas, Ruben and Ellison, Aaron M. and Blouin, Manuel and Scheu, Stefan and Freschet, Grégoire T. and Rose, Laura and Barot, Sebastien and Cesarz, Simone and Eisenhauer, Nico and Girin, Thomas and Assandri, Davide and Bonkowski, Michael and Buchmann, Nina and Butenschoen, Olaf and Devidal, Sebastien and Gleixner, Gerd and Gessler, Arthur and Gigon, Agnès and Greiner, Anna and Grignani, Carlo and Hansart, Amandine and Kayler, Zachary and Lange, Markus and Lata, Jean-Christophe and Le Galliard, Jean-François and Lukac, Martin and Mannerheim, Neringa and Müller, Marina E. H. and Pando, Anne and Rotter, Paula and Scherer-Lorenzen, Michael and Seyhun, Rahme and Urban-Mead, Katherine and Weigelt, Alexandra and Zavattaro, Laura and Roy, Jacques},
	month = feb,
	year = {2018},
	pages = {279--287},
	file = {Milcu et al. - 2018 - Genotypic variability enhances the reproducibility.pdf:C\:\\Users\\jorda\\Zotero\\storage\\YD5JNGBP\\Milcu et al. - 2018 - Genotypic variability enhances the reproducibility.pdf:application/pdf},
}

@article{pennisi_spider_2020,
	title = {Spider biologist denies suspicions of widespread data fraud in his animal personality research},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/news/2020/01/spider-biologist-denies-suspicions-widespread-data-fraud-his-animal-personality},
	doi = {10.1126/science.abb1258},
	urldate = {2022-04-22},
	journal = {Science},
	author = {Pennisi, Elizabeth},
	month = jan,
	year = {2020},
}

@article{fanelli_is_2018,
	title = {Is science really facing a reproducibility crisis, and do we need it to?},
	volume = {115},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1708272114},
	doi = {10.1073/pnas.1708272114},
	number = {11},
	urldate = {2022-04-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fanelli, Daniele},
	month = mar,
	year = {2018},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {2628--2631},
	file = {Full Text PDF:C\:\\Users\\jorda\\Zotero\\storage\\KB6R3MZ8\\Fanelli - 2018 - Is science really facing a reproducibility crisis,.pdf:application/pdf},
}

@article{elliott_taxonomy_2020,
	title = {A {Taxonomy} of {Transparency} in {Science}},
	issn = {0045-5091, 1911-0820},
	url = {https://www.cambridge.org/core/product/identifier/S0045509120000211/type/journal_article},
	doi = {10.1017/can.2020.21},
	abstract = {Both scientists and philosophers of science have recently emphasized the importance of promoting transparency in science. For scientists, transparency is a way to promote reproducibility, progress, and trust in research. For philosophers of science, transparency can help address the value-ladenness of scientific research in a responsible way. Nevertheless, the concept of transparency is a complex one. Scientists can be transparent about many different things, for many different reasons, on behalf of many different stakeholders. This paper proposes a taxonomy that clarifies the major dimensions along which approaches to transparency can vary. By doing so, it provides several insights that philosophers and other science studies scholars can pursue. In particular, it helps address common objections to pursuing transparency in science, it clarifies major forms of transparency, and it suggests avenues for further research on this topic.},
	language = {en},
	urldate = {2022-04-23},
	journal = {Canadian Journal of Philosophy},
	author = {Elliott, Kevin C.},
	month = jun,
	year = {2020},
	pages = {1--14},
	file = {Elliott - 2020 - A Taxonomy of Transparency in Science.pdf:C\:\\Users\\jorda\\Zotero\\storage\\TGR9YMT7\\Elliott - 2020 - A Taxonomy of Transparency in Science.pdf:application/pdf},
}

@article{wicherts_letting_2012,
	title = {Letting the daylight in: {Reviewing} the reviewers and other ways to maximize transparency in science},
	volume = {6},
	issn = {1662-5188},
	shorttitle = {Letting the daylight in},
	url = {https://www.frontiersin.org/article/10.3389/fncom.2012.00020},
	abstract = {With the emergence of online publishing, opportunities to maximize transparency of scientific research have grown considerably. However, these possibilities are still only marginally used. We argue for the implementation of (1) peer-reviewed peer review, (2) transparent editorial hierarchies, and (3) online data publication. First, peer-reviewed peer review entails a community-wide review system in which reviews are published online and rated by peers. This ensures accountability of reviewers, thereby increasing academic quality of reviews. Second, reviewers who write many highly regarded reviews may move to higher editorial positions. Third, online publication of data ensures the possibility of independent verification of inferential claims in published papers. This counters statistical errors and overly positive reporting of statistical results. We illustrate the benefits of these strategies by discussing an example in which the classical publication system has gone awry, namely controversial IQ research. We argue that this case would have likely been avoided using more transparent publication practices. We argue that the proposed system leads to better reviews, meritocratic editorial hierarchies, and a higher degree of replicability of statistical analyses.},
	urldate = {2022-04-23},
	journal = {Frontiers in Computational Neuroscience},
	author = {Wicherts, Jelte and Kievit, Rogier and Bakker, Marjan and Borsboom, Denny},
	year = {2012},
	file = {Full Text PDF:C\:\\Users\\jorda\\Zotero\\storage\\3JR4RRRT\\Wicherts et al. - 2012 - Letting the daylight in Reviewing the reviewers a.pdf:application/pdf},
}

@article{begley_reproducibility_2015,
	title = {Reproducibility in {Science}},
	volume = {116},
	url = {https://www.ahajournals.org/doi/full/10.1161/CIRCRESAHA.114.303819},
	doi = {10.1161/CIRCRESAHA.114.303819},
	abstract = {Medical and scientific advances are predicated on new knowledge that is robust and reliable and that serves as a solid foundation on which further advances can be built. In biomedical research, we are in the midst of a revolution with the generation of new data and scientific publications at a previously unprecedented rate. However, unfortunately, there is compelling evidence that the majority of these discoveries will not stand the test of time. To a large extent, this reproducibility crisis in basic and preclinical research may be as a result of failure to adhere to good scientific practice and the desperation to publish or perish. This is a multifaceted, multistakeholder problem. No single party is solely responsible, and no single solution will suffice. Here we review the reproducibility problems in basic and preclinical biomedical research, highlight some of the complexities, and discuss potential solutions that may help improve research quality and reproducibility.},
	number = {1},
	urldate = {2022-04-23},
	journal = {Circulation Research},
	author = {Begley, C. Glenn and Ioannidis, John P.A.},
	month = jan,
	year = {2015},
	note = {Publisher: American Heart Association},
	keywords = {funding, journals, research integrity, universities},
	pages = {116--126},
	file = {Full Text PDF:C\:\\Users\\jorda\\Zotero\\storage\\LSEVZYAK\\Begley et Ioannidis - 2015 - Reproducibility in Science.pdf:application/pdf},
}

@article{eisner_reproducibility_2018,
	title = {Reproducibility of science: {Fraud}, impact factors and carelessness},
	volume = {114},
	issn = {0022-2828},
	shorttitle = {Reproducibility of science},
	url = {https://www.sciencedirect.com/science/article/pii/S0022282817303334},
	doi = {10.1016/j.yjmcc.2017.10.009},
	abstract = {There is great concern that results published in a large fraction of biomedical papers may not be reproducible. This article reviews the evidence for this and considers some of the factors that are responsible and how the problem may be solved. One issue is scientific fraud. This, in turn, may result from pressures put on scientists to succeed including the need to publish in “high impact” journals. I emphasise the importance of judging the quality of the science itself as opposed to using surrogate metrics. The other factors discussed include problems of experimental design and statistical analysis of the work. It is important that these issues are addressed by the scientific community before others impose draconian regulations.},
	language = {en},
	urldate = {2022-04-24},
	journal = {Journal of Molecular and Cellular Cardiology},
	author = {Eisner, D. A.},
	month = jan,
	year = {2018},
	keywords = {Fraud, Reproducibility},
	pages = {364--368},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jorda\\Zotero\\storage\\R26E8CUC\\Eisner - 2018 - Reproducibility of science Fraud, impact factors .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jorda\\Zotero\\storage\\LVLEH9BL\\S0022282817303334.html:text/html},
}

@article{baker_1500_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	copyright = {2016 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/533452a},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	language = {en},
	number = {7604},
	urldate = {2022-04-24},
	journal = {Nature},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	note = {Number: 7604
Publisher: Nature Publishing Group},
	keywords = {Peer review, Publishing, Research management},
	pages = {452--454},
	file = {Full Text PDF:C\:\\Users\\jorda\\Zotero\\storage\\TCQICZVH\\Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf:application/pdf},
}
